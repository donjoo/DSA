

'''

üß† What is Dynamic Programming?

Dynamic Programming is an optimization technique used to solve problems by breaking 
them into overlapping subproblems, solving each subproblem only once, and storing their 
results to avoid recomputation.

üßÆ 1. Top-Down (Memoization)
üîß Concept

    Use recursion + caching

    Save results of subproblems in a dictionary/list

    Avoids repeated calculations


üîª 2. Bottom-Up (Tabulation)
üîß Concept

    Solve the smallest subproblems first

    Build up to the final solution

    Iterative approach with a table/array



    You said:
you are a DSA expert . i am asking things realated to DSA..  give everthing on the topic i am asking
ChatGPT said:

Absolutely! I'm ready ‚Äî ask me anything related to Data Structures and Algorithms (DSA), and I‚Äôll give you:

    Definition and concept

    Real-world analogy (if applicable)

    Types or variations

    Key operations

    Time & space complexity

    Python code examples

    Use cases

    Common interview questions (if you want)

Go ahead and tell me the DSA topic you're currently on!
You said:
Search
ChatGPT said:

Sure! It sounds like you want to dive into the topic of search algorithms in DSA. Here‚Äôs a comprehensive breakdown:
üîç 1. Definition & Concept

Search algorithms are techniques for finding a target value (or determining its absence) within a data structure‚Äîlike an array, tree, graph, or hash table.
üîç 2. Common Search Types
a. Linear Search

    Concept: Scan each element one by one until you find the target.

    Time Complexity: O(n)

    Space Complexity: O(1)

    Use Case: Unsorted arrays or small datasets.

def linear_search(arr, target):
    for i, val in enumerate(arr):
        if val == target:
            return i
    return -1

b. Binary Search

    Concept: Repeatedly divide a sorted array in half. Check mid, decide left/right.

    Time Complexity: O(log n)

    Space Complexity: O(1) iterative or O(log n) recursive

    Use Case: Only on sorted arrays.

def binary_search(arr, target):
    low, high = 0, len(arr) - 1
    while low <= high:
        mid = (low + high) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            low = mid + 1
        else:
            high = mid - 1
    return -1

c. Ternary Search

    Concept: Similar to binary but splits into three parts each iteration.

    Time Complexity: O(log‚ÇÉ n) ‚âà O(log n)

    Use Case: Rare‚Äîusually used for unimodal functions in optimization.

d. Interpolation Search

    Concept: Predicts position of target using its value relative to array‚Äôs range.

    Time Complexity: O(log log n) on uniformly distributed data

    Worst Case: O(n)

    Use Case: Large, uniformly distributed sorted arrays.

e. Exponential (Galloping) Search

    Concept: Repeatedly double range until target is exceeded, then binary search.

    Time Complexity: O(log n)

    Use Case: Unbounded or huge sorted arrays.

üîç 3. Search in Trees
a. Binary Search Tree (BST)

    Search Time: O(h) where h is tree height (balanced: O(log n), skewed: O(n)).

    Process: Compare, go left/right recursively.

b. Balanced Trees

    Examples: AVL, Red-Black

    Search: O(log n)

c. Trie (Prefix Tree)

    Use Case: Word dictionaries, auto-complete

    Time: O(m), where m = length of key

üîç 4. Graph Search Algorithms
a. Breadth‚ÄëFirst Search (BFS)

    Layer-by-layer traversal

    Time: O(V + E)

    Use: Shortest path in unweighted graphs

b. Depth‚ÄëFirst Search (DFS)

    Explore deep paths using recursion/stack

    Time: O(V + E)

    Use: Cycle detection, connectivity

def dfs(graph, start, visited=None):
    if visited is None:
        visited = set()
    visited.add(start)
    for neighbor in graph[start]:
        if neighbor not in visited:
            dfs(graph, neighbor, visited)
    return visited

üîç 5. Hash‚Äëbased Search

    Concept: Use hash tables to achieve average O(1) lookup.

    Worst Case: O(n) in case of collisions.

üîç 6. Comparison Table
Algorithm	Type	Best Time	Worst Time	Use Case
Linear Search	Array	O(1)	O(n)	Small or unsorted data
Binary Search	Sorted	O(log n)	O(log n)	Large sorted arrays
Interpolation Search	Sorted	O(log log n)	O(n)	Uniformly distributed data
Exponential Search	Sorted	O(log n)	O(log n)	Unbounded sorted arrays
BST Search	Tree	O(log n)	O(n)	BST (balanced)
Trie Search	String	O(m)	O(m)	Dictionary, auto-complete
BFS/DFS	Graph	O(V + E)	O(V + E)	Graph traversal & path problems
Hash Table	Any	O(1)	O(n)	General fast lookup scenarios
üîç 7. Interview Tips & Questions

    Explain why binary search requires sorted data.

    Discuss pitfalls: mid overflow (mid = low + (high ‚àí low)//2).

    Compare BFS vs DFS: Which is better for shortest paths?

    Explain hashing collision handling (chaining vs open addressing).

üîç 8. Real‚ÄëWorld Analogies

    Linear Search: Looking for a person in a line by asking each person.

    Binary Search: Telephone book lookup‚Äîgo to the middle and decide to go up/down.

    Trie: Organizing a dictionary by spelling, letter by letter.

Let me know if you‚Äôd like code for any specific data structure or deeper into graphs, tries, interpolation search, or more advanced topics like search in external memory, search optimizations, or probabilistic search!
You said:
Binary Search(recursive also)
ChatGPT said:

Perfect! Let‚Äôs dive into Binary Search, including both iterative and recursive approaches.
üîé Binary Search
üß† Concept

Binary Search is a divide and conquer algorithm. It works only on sorted arrays or lists. The idea is to repeatedly divide the search interval in half:

    Compare the target with the middle element.

    If equal ‚Üí found.

    If target < middle ‚Üí search in left half.

    If target > middle ‚Üí search in right half.

‚úÖ Requirements

    Input must be sorted.

    Efficient for large datasets.

‚è±Ô∏è Time & Space Complexity
Case	Time Complexity
Best	O(1)
Average	O(log n)
Worst	O(log n)

Space:

    Iterative: O(1)

    Recursive: O(log n) (due to recursion stack)

üß™ Iterative Binary Search (Python)

def binary_search_iterative(arr, target):
    low = 0
    high = len(arr) - 1

    while low <= high:
        mid = (low + high) // 2  # OR use: low + (high - low) // 2 to avoid overflow

        if arr[mid] == target:
            return mid  # Target found
        elif arr[mid] < target:
            low = mid + 1  # Go right
        else:
            high = mid - 1  # Go left

    return -1  # Not found

üîÅ Recursive Binary Search

def binary_search_recursive(arr, target, low, high):
    if low > high:
        return -1  # Base case: not found

    mid = (low + high) // 2

    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search_recursive(arr, target, mid + 1, high)
    else:
        return binary_search_recursive(arr, target, low, mid - 1)

üëá How to call:

arr = [2, 4, 6, 8, 10, 12, 14]
target = 10

# Iterative
print(binary_search_iterative(arr, target))  # Output: 4

# Recursive
print(binary_search_recursive(arr, target, 0, len(arr)-1))  # Output: 4

üß† Visual Example

Given arr = [1, 3, 5, 7, 9, 11] and target = 5:

    First mid = index 2 ‚Üí arr[2] = 5 ‚úÖ Found

Given target = 9:

    mid = index 2 ‚Üí arr[2] = 5 ‚Üí go right

    new mid = index 4 ‚Üí arr[4] = 9 ‚úÖ Found

üõ†Ô∏è Edge Cases

    Empty array ‚Üí return -1

    Target at edges (start or end)

    Duplicates? ‚Üí returns an index of target, not necessarily the first/last

üìå Common Interview Follow-ups

    What if the array is decreasing?
    ‚Üí Reverse the direction of comparisons.

    How to find the first or last occurrence of a target (with duplicates)?
    ‚Üí Use modified binary search with boundary checks.

    Can we use binary search on linked lists?
    ‚Üí Not efficient. Linked lists don‚Äôt have direct access to middle elements (O(n) access time).

üìö Use Cases

    Efficient lookups in sorted arrays

    Searching in dictionaries (underlying binary trees)

    Solving algorithm problems:

        Ceiling / Floor in sorted array

        Rotated sorted array search

        Peak element search

        Binary search on the answer (advanced use)

Would you like:

    Modified versions of binary search (like finding first/last occurrence)?

    Binary Search in rotated arrays?

    Binary Search applied to more advanced problems (e.g., LeetCode questions)?

Just let me know!
You said:
Linear Search
ChatGPT said:

Great! Let's explore Linear Search, one of the simplest and most fundamental searching algorithms in DSA.
üîç Linear Search
üß† Concept

Linear Search (also called sequential search) scans each element one-by-one in the list or array until it finds the target value or reaches the end.
‚úÖ Characteristics

    Does not require sorted data

    Works on arrays, lists, strings, linked lists, etc.

    Simple but inefficient for large datasets

‚è±Ô∏è Time & Space Complexity
Case	Time Complexity
Best	O(1) (target is first element)
Average	O(n)
Worst	O(n) (target is last or not present)

Space Complexity: O(1) ‚Äî no extra space needed
üß™ Python Code
1. Linear Search in a List

def linear_search(arr, target):
    for index, value in enumerate(arr):
        if value == target:
            return index  # Target found
    return -1  # Not found

Example Usage

arr = [10, 20, 30, 40, 50]
target = 30

print(linear_search(arr, target))  # Output: 2

2. Linear Search in a String

def linear_search_string(s, target_char):
    for index, char in enumerate(s):
        if char == target_char:
            return index
    return -1

3. Linear Search in a Linked List (conceptual)

class Node:
    def __init__(self, data):
        self.data = data
        self.next = None

def linear_search_linked_list(head, target):
    index = 0
    current = head
    while current:
        if current.data == target:
            return index
        current = current.next
        index += 1
    return -1

üìå When to Use Linear Search

    When the dataset is small

    When the data is unsorted

    When insertion order is preserved (like linked lists)

    When constant time preprocessing is required

üß† Real-World Analogy

    Looking for your friend in a crowd by checking every single face one by one.

‚ùó Drawbacks

    Inefficient for large datasets (O(n))

    Not suitable when faster methods like binary search can be applied

üìö Variations

    Search all occurrences:

def all_occurrences(arr, target):
    return [i for i, val in enumerate(arr) if val == target]

    Search with early exit (standard linear search)

    Search using in keyword in Python:

if target in arr:
    print("Found")

üéØ Interview Questions

    What is the difference between linear and binary search?

    Why would you choose linear search over binary search?

    How would you search for all matching elements?

Let me know if you want:

    Linear search for objects/dictionaries

    Comparison with other search algorithms

    More real-world problems using linear search

Just say the word!
You said:

Why would you choose linear search over binary search?
?
ChatGPT said:

Great question! Here's why you would choose Linear Search over Binary Search, even though binary search is generally faster:
‚úÖ When to Use Linear Search Instead of Binary Search
1. üîÄ When the data is unsorted

    Binary search requires sorted data.

    Linear search works on any data order.

üìå Example: [12, 3, 7, 4, 18] ‚Üí You must use linear search here unless you sort it first.
2. üî¢ When the dataset is small

    For small lists, the performance difference between O(n) and O(log n) is negligible.

    Linear search is simpler and has lower overhead.

3. üß† When you want to find multiple matches

    Linear search can easily find all occurrences of a value.

    Binary search only finds one match ‚Äî and even then, not guaranteed to be the first or last without modification.

4. üîÑ When the data is dynamic

    If the data changes frequently, it's expensive to keep it sorted (which binary search needs).

    Linear search can handle real-time insertions/deletions without maintenance.

5. üß± When working with linked lists or sequential-access structures

    Binary search is inefficient on structures like linked lists (no random access).

    Linear search is ideal because you can traverse node by node.

6. üõ†Ô∏è Ease of implementation

    Simpler code, especially for quick prototyping or one-off use.

üîÅ Summary Table
Criteria	Linear Search ‚úÖ	Binary Search ‚ùå
Works on unsorted data	‚úÖ	‚ùå
Simple to implement	‚úÖ	‚ùå
Works on small datasets	‚úÖ	‚úÖ
Finds all occurrences	‚úÖ	‚ùå
Works on linked lists	‚úÖ	‚ùå
Requires sorting	‚ùå	‚úÖ

Let me know if you want code examples comparing both or real-world problems where linear search is the better fit!
You said:
Recursion
ChatGPT said:

Let's explore Recursion, one of the most important and frequently asked topics in DSA.
üîÅ What is Recursion?

Recursion is a programming technique where a function calls itself to solve smaller instances of the same problem.
üß† Key Idea

A recursive function solves a problem by:

    Base Case: A condition to stop recursion.

    Recursive Case: Calling itself on a smaller subproblem.

üìå Real-Life Analogy

    A set of nested Russian dolls ‚Äî you keep opening one until you reach the smallest (base case).

‚úÖ Syntax in Python

def recursive_function(parameters):
    if base_case_condition:
        return base_case_result
    else:
        return recursive_function(smaller_problem)

üîÅ Example 1: Factorial (Classic Recursion)

def factorial(n):
    if n == 0 or n == 1:
        return 1  # Base case
    return n * factorial(n - 1)  # Recursive case

print(factorial(5))  # Output: 120

üîÅ Example 2: Fibonacci Sequence

def fibonacci(n):
    if n <= 1:
        return n  # Base cases: 0 or 1
    return fibonacci(n - 1) + fibonacci(n - 2)

    ‚ö†Ô∏è This has exponential time complexity. Use memoization or iteration for optimization.

üîÅ Example 3: Sum of a List

def sum_list(arr):
    if not arr:
        return 0  # Base case
    return arr[0] + sum_list(arr[1:])  # Recursive case

üìâ Time & Space Complexity

    Time depends on the number of recursive calls

    Space is usually O(depth of recursion) due to the call stack

‚ö†Ô∏è Stack Overflow

Too many recursive calls can lead to:

RecursionError: maximum recursion depth exceeded

üõ†Ô∏è Solution:

    Use tail recursion (if supported) or convert to iteration

üìå Tail Recursion

A special form of recursion where the recursive call is the last operation in the function. Python does not optimize tail recursion.
üîÑ Recursive vs Iterative
Feature	Recursive	Iterative
Simplicity	Easier for tree/graph problems	Often more efficient
Memory	Uses call stack	Uses loops & less memory
Readability	Clean & elegant	Verbose for complex logic
Speed	May be slower	Usually faster
üß± Common Problems Solved Using Recursion
Problem	Description
Factorial	n! = n √ó (n - 1)!
Fibonacci	F(n) = F(n-1) + F(n-2)
Binary Search (recursive)	Divide-and-conquer on sorted array
Tower of Hanoi	Move disks recursively
Tree Traversals	Inorder, Preorder, Postorder
DFS on Graph	Explore nodes via recursion
Backtracking problems	N-Queens, Sudoku, etc.
Subsets / Combinations	Recursive generation of solutions
üß† Tip for Writing Recursive Code

    Identify the base case

    Break the problem into smaller subproblems

    Combine results from recursive calls

    Make sure recursion reduces the problem towards the base case

Would you like:

    Visual diagrams of how recursion works?

    More practice problems with recursion?

    Recursion applied to trees, graphs, or backtracking?

Let me know your next direction!
You said:
Iterative & recursive
ChatGPT said:

You're asking for Iterative vs Recursive approaches ‚Äî a classic DSA topic! Let‚Äôs break it down fully and compare them with examples, pros/cons, and use cases.
üîÅ What Are They?
‚úÖ Recursive Approach

A function calls itself to solve a smaller part of the problem.
‚úÖ Iterative Approach

Uses loops (for/while) to repeat operations until the condition is met.
üÜö Key Differences
Feature	Recursive	Iterative
Control Flow	Function calls itself	Uses loops (for, while)
Memory Usage	More (uses call stack)	Less (uses loop control vars)
Performance	Sometimes slower	Usually faster
Code Complexity	Simple, elegant (especially for trees)	Longer, sometimes harder to read
Stack Overflow Risk	Yes (deep recursion)	No stack overflow
üîÅ Example 1: Factorial
Recursive

def factorial_recursive(n):
    if n == 0 or n == 1:
        return 1
    return n * factorial_recursive(n - 1)

Iterative

def factorial_iterative(n):
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result

üîÅ Example 2: Fibonacci
Recursive

def fibonacci_recursive(n):
    if n <= 1:
        return n
    return fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)

Iterative

def fibonacci_iterative(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

    ‚úÖ Iterative version is much more efficient (O(n)) than naive recursion (O(2‚Åø)).

üîÅ Example 3: Binary Search
Recursive

def binary_search_recursive(arr, target, low, high):
    if low > high:
        return -1
    mid = (low + high) // 2
    if arr[mid] == target:
        return mid
    elif arr[mid] > target:
        return binary_search_recursive(arr, target, low, mid - 1)
    else:
        return binary_search_recursive(arr, target, mid + 1, high)

Iterative

def binary_search_iterative(arr, target):
    low, high = 0, len(arr) - 1
    while low <= high:
        mid = (low + high) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] > target:
            high = mid - 1
        else:
            low = mid + 1
    return -1

‚úÖ When to Use What?
Use Recursion When:

    Problem is naturally recursive (trees, graphs, divide & conquer)

    Code clarity is more important than performance

    You want cleaner, shorter code

Use Iteration When:

    Performance and memory usage matter

    Problem doesn‚Äôt need call stack (simple loops work fine)

    Stack overflow is a risk due to deep recursion

üîÅ Convert Recursion to Iteration

Many recursive solutions can be converted using explicit stacks.

Example: DFS on a graph/tree

# Recursive
def dfs_recursive(node, visited):
    visited.add(node)
    for neighbor in node.neighbors:
        if neighbor not in visited:
            dfs_recursive(neighbor, visited)

# Iterative
def dfs_iterative(start):
    visited = set()
    stack = [start]
    while stack:
        node = stack.pop()
        if node not in visited:
            visited.add(node)
            for neighbor in reversed(node.neighbors):
                stack.append(neighbor)

üß† Summary
Aspect	Recursive	Iterative
Space	O(n) stack	O(1) (unless using own stack)
Readability	Short, clean	Can be longer
Speed	Sometimes slower	Generally faster
Risk	Stack overflow	None
When to use	Trees, graphs, divide & conquer	Loops, calculations, optimization
Want to Practice?

I can give you DSA problems where you:

    Solve using both recursion and iteration

    Convert recursive to iterative

    Use recursion for trees and backtracking

Let me know what direction you'd like to take next!
You said:
Virtual memory
ChatGPT said:

Let‚Äôs dive deep into Virtual Memory ‚Äî a critical concept in computer architecture and operating systems that is often asked in system design, OS fundamentals, and DSA-adjacent interviews.
üß† What is Virtual Memory?

Virtual Memory is a memory management technique that provides an "illusion" of a large, contiguous block of memory to each process, even if the physical memory (RAM) is smaller or fragmented.

    Each process thinks it has its own large block of memory, but it is actually mapped to physical memory and disk space.

üß© Why Virtual Memory?

    Multitasking: Multiple processes can run without interfering with each other.

    Security: One process can't access another's memory.

    Abstraction: Hides physical memory layout.

    Flexibility: Programs can use more memory than physically available (via disk).

    Efficiency: Enables techniques like paging, caching, and lazy loading.

üèóÔ∏è How It Works

Virtual memory uses a combination of:

    Physical Memory (RAM)

    Disk storage (called swap space or page file)

The OS uses a Page Table to map:

Virtual Address ‚Üí Physical Address (RAM or Disk)

üì¶ Key Components
1. Pages

    Virtual memory is divided into fixed-size blocks called pages (e.g., 4 KB each).

2. Frames

    Physical memory (RAM) is also divided into frames (same size as pages).

3. Page Table

    A data structure maintained by the OS that maps virtual pages to physical frames or disk locations.

üîÑ Page Fault

When a process accesses a page not in RAM:

    Page Fault occurs.

    OS retrieves the page from disk (swap space).

    If RAM is full, one page is swapped out using a Page Replacement Algorithm.

üîÅ Page Replacement Algorithms
Algorithm	Description
FIFO	Oldest page removed
LRU (Least Recently Used)	Least recently accessed page removed
Optimal	Removes page that won‚Äôt be used soon
üßÆ Address Translation

Virtual Address is split into:

    Page number ‚Üí Index into the page table

    Offset ‚Üí Position within the page

Page table entry provides the frame number, and physical address = frame base + offset
‚ö° Performance Boost: TLB (Translation Lookaside Buffer)

    A cache for recent page table lookups

    Reduces memory access time

    If TLB miss ‚Üí access page table (slower)

üîí Advantages

    Program isolation (security)

    Allows large programs on small memory systems

    Efficient use of physical memory

    Simplifies programming (no need to manage memory manually)

‚ö†Ô∏è Disadvantages

    Overhead from address translation

    Page faults are expensive (disk access is slow)

    Thrashing can occur if memory is too small (constant swapping)

üí° Real-World Analogy

    Imagine you have a small desk (RAM), but your documents (programs) are stored in a big filing cabinet (Disk). You bring in only the pages you need to work on. If the desk is full, you put a page back into the cabinet to make space.

üìö Interview Tips

    What is virtual memory and how does it help?

    What is a page fault?

    Explain how address translation works.

    Difference between physical and virtual address.

    How does TLB improve performance?

    What happens when there‚Äôs not enough physical memory?

üõ†Ô∏è Summary
Term	Description
Virtual Memory	Illusion of large memory
Page	Virtual memory block
Frame	Physical memory block
Page Table	Maps virtual ‚Üí physical address
Page Fault	Page not in memory ‚Üí fetched from disk
TLB	Fast lookup for recent translations

Would you like:

    A visual diagram of how virtual memory works?

    Code simulation of paging in Python?

    MCQs or interview-style questions to practice?

Let me know how deep you want to go!
You said:
Dynamic programing

    Memoize approach
    Bottom up approach
ChatGPT said:

Excellent! Let's dive into Dynamic Programming (DP) ‚Äî specifically the two most popular techniques used to solve DP problems:
üß† What is Dynamic Programming?

Dynamic Programming is an optimization technique used to solve problems by breaking them into overlapping subproblems, solving each subproblem only once, and storing their results to avoid recomputation.
üîÅ Two Main Approaches:
1. Top-Down (Memoization)
2. Bottom-Up (Tabulation)

We'll look at both in detail.
üì¶ Common DP Characteristics

A problem can be solved using DP if it has:

    Overlapping Subproblems

    Optimal Substructure

üß≠ Problem Example We'll Use

Let‚Äôs use the classic Fibonacci sequence to explain both approaches.

Fibonacci:

F(n) = F(n-1) + F(n-2)
Base: F(0) = 0, F(1) = 1

üßÆ 1. Top-Down (Memoization)
üîß Concept

    Use recursion + caching

    Save results of subproblems in a dictionary/list

    Avoids repeated calculations

‚úÖ Code (Memoization in Python)

def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]

üß† How It Works

    fib_memo(5) breaks into fib(4) and fib(3)

    Those break down further...

    Already computed results are stored and reused

‚è±Ô∏è Time & Space

    Time Complexity: O(n)

    Space Complexity: O(n) for recursion stack + memo table

üîª 2. Bottom-Up (Tabulation)
üîß Concept

    Solve the smallest subproblems first

    Build up to the final solution

    Iterative approach with a table/array

‚úÖ Code (Bottom-Up in Python)

def fib_bottom_up(n):
    if n <= 1:
        return n
    dp = [0] * (n + 1)
    dp[1] = 1
    for i in range(2, n + 1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]

üöÄ Optimized Space Version

def fib_optimized(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

‚è±Ô∏è Time & Space

    Time Complexity: O(n)

    Space Complexity: O(n) or O(1) (optimized)

üÜö Memoization vs Bottom-Up
Feature	Memoization (Top-Down)	Bottom-Up (Tabulation)
Code style	Recursive	Iterative
Space (stack)	O(n) recursion stack	O(1) if optimized
Complexity	O(n)	O(n)
Readability	Clean, natural	More verbose sometimes
Good for	Tree-like recursion	When order is import
'''



def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]








def fib_bottom_up(n):
    if n <= 1:
        return n
    db = [0] * (n + 1)
    db[1] =1
    for i in range(2,n + 1):
        db[i] = db[i -1] + db[i-2]
    return db[n]
    
    
print(fib_bottom_up(7))

def fib_optimezed(n):
    if n <= 1:
        return n
    a,b = 0,1
    for _ in range(2, n + 1):
        a,b = b, a + b
    return b
    
    
print(fib_optimezed(7))